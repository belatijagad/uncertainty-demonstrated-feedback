# DITTO Authors: Omar Shaikh, Michelle S. Lam, Joey Hejna, Yijia Shao, Hyundong Cho, Michael S. Bernstein, Diyi Yang
# Copyright 2020-2025 The HuggingFace Team. All rights reserved.

import random
from typing import Any
from dataclasses import dataclass, field

import torch
from datasets import Dataset, IterableDataset
from transformers import PreTrainedModel, PreTrainedTokenizerBase
from trl.trainer.dpo_trainer import DataCollatorForPreference
from trl.trainer.utils import pad

from scripts.utils import generate_model_outputs
from scripts.estimator import BaseEstimator

@dataclass
class DITTOCollator(DataCollatorForPreference):
    """
    DITTO DataCollator that handles dynamic sampling of generated responses 
    (Expert, Replay, Noisy) and correctly formats them for DPO training.
    """
    frac_expert: float = 0.7
    frac_replay: float = 0.2
    frac_noisy: float = 0.1
    rescale_batch: int = 2
    higher_is_better: bool = False
    gen_kwargs: dict = field(default_factory=dict)

    tokenizer: PreTrainedTokenizerBase = None
    estimator: BaseEstimator = None

    # dict[timestep: int, dict[ prompt: str, list[ outputs: dict[str, torch.tensor] ] ] ]
    cache: dict[int, dict[str, list[ dict[str, torch.Tensor] ]]] = field(default_factory=dict)
    sampled_step: int = field(default=0, init=False)

    def __post_init__(self):
        if self.tokenizer is None:
            raise ValueError("Tokenizer must be provided to `DITTOCollator`.")
        
        if self.pad_token_id == None:
            self.pad_token_id = self.tokenizer.pad_token_id

    def set_mode(self, *, training: bool) -> None:
        self.mode = "train" if training else "eval"

    def resample(
        self,
        step: int,
        model: PreTrainedModel,
        tokenizer: PreTrainedTokenizerBase,
        dataset: Dataset | IterableDataset,
    ) -> None:
        """
        Generates new responses from the model and updates the cache.
        """
        self.sampled_step = step
        self.cache.setdefault(step, {})

        prompts = list(dataset["prompt"])
        
        # TODO: finalize the utils generation
        (
            prompt_input_ids, 
            generation_input_ids, 
            scores_view, 
            logits_view
        ) = generate_model_outputs(
            prompts=prompts,
            model=model,
            tokenizer=tokenizer,
            gen_kwargs=self.gen_kwargs,
        )

        for prompt, pr_ids, gen_ids, scores, logits in zip(
            prompts,
            prompt_input_ids, generation_input_ids,
            scores_view, logits_view, strict=True
        ):
            cache_slot = self.cache[step].setdefault(prompt, [])

            # For each generated sequences for the same prompt
            for pr_id, gen_id, score, logit in zip(
                pr_ids, gen_ids, scores, logits, strict=True
            ):
                cache_slot.append(
                    {
                        "score": self.estimator(gen_id, score, logit),
                        "prompt_input_ids": pr_id,
                        "generated_input_ids": gen_id,
                    }
                )
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    def _get_noisy_pairs(
            self, 
            prompt: list[torch.Tensor], 
            step_a: int,
        ) -> list[tuple]:
        """
        Generates 'noisy' preference pairs by comparing scores of different generations.
        """
        noisy_pairs = []

        step_data = self.cache.get(step_a, {})
        if prompt not in step_data:
            return noisy_pairs

        current_entries = self.cache[step_a][prompt]
        prompt_input_ids = current_entries[0]["prompt_input_ids"]
        
        # Look at all previous steps
        for step_b in range(step_a):
            past_entries = self.cache.get(step_b, {}).get(prompt)
            if not past_entries:
                continue

            for current in current_entries:
                for past in past_entries:
                    curr_score, past_score = float(current["score"]), float(past["score"])
                    past_is_better = (
                        (past_score > curr_score) 
                        if self.higher_is_better 
                        else (past_score < curr_score)
                    )
                    final_chosen = past if past_is_better else current
                    final_rejected = current if past_is_better else past

                    noisy_pairs.append((
                        prompt_input_ids, 
                        final_chosen["generation_input_ids"], 
                        final_rejected["generation_input_ids"]
                    ))

        return noisy_pairs

    def torch_call(self, examples: list[dict[str, Any]]) -> dict[str, Any]:
        def attn_mask(input_ids: list[torch.Tensor]) -> list[torch.Tensor]:
            return [torch.ones_like(input_id) for input_id in input_ids]
        
        expert_samples = []
        replay_samples = []
        noisy_samples = []

        for example in examples:
            prompt_text = example["prompt"]
            # Expert samples
            for rejected in self.cache[self.sampled_step][prompt_text]:
                expert_samples.append((
                    example["prompt_input_ids"],
                    example["chosen_input_ids"],
                    rejected["generation_input_ids"],
                ))

            assert expert_samples
            
            for step_a in self.cache.keys():
                # Replay samples
                if step_a < self.sampled_step:
                    for rejected in self.cache[self.sampled_step][prompt_text]:
                        replay_samples.append((
                            example["prompt_input_ids"],
                            example["chosen_input_ids"],
                            rejected["generation_input_ids"],
                        ))
                # Noisy samples
                noisy_samples.extend(self._get_noisy_pairs(prompt_text, step_a))
        
        # Sample outputs
        n_expert = int(len(expert_samples) * self.frac_expert)
        n_replay = int(len(replay_samples) * self.frac_replay)
        n_noisy  = int(len(noisy_samples)  * self.frac_noisy )
        samples  = (
            random.sample(expert_samples, min(len(expert_samples), n_expert)) +
            random.sample(replay_samples, min(len(replay_samples), n_replay)) +
            random.sample(noisy_samples,  min(len(noisy_samples),  n_noisy))
        )

        # Form output
        keys = ["prompt_input_ids", "chosen_input_ids", "rejected_input_ids"]
        output = {
            key: [sample[i] for sample in samples]
            for i, key in enumerate(keys)
        }
        
        # Create attn mask and pad
        for name, ids in output.keys():
            output[name] = pad(ids, padding_value=self.tokenizer.pad_token_id, padding_side="left")
            output[f"{name.split["_"][0]}_attention_mask"] = pad(attn_mask(ids), padding_value=0, padding_side="left" if "prompt" in name else "right")
        
        return output
