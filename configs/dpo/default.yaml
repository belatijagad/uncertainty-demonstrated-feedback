commit_message: "Test"
seed: 42

model:
  name_or_path: "mistralai/Ministral-8B-Instruct-2410"
  use_bf16: true

wandb:
  enabled: true
  project: "uncertainty-aware-alignment"
  name: "ministral8binstruct_dpo"
  group: ["dpo"]
  tags: ["ministral", "8b"]
  notes: "DPO training on Ministral-8B-Instruct-2410"

enable_lora: true
lora:
  lora_r: 32
  lora_alpha: 64
  lora_dropout: 0.05
  lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj

dataset:
  name_or_path: "HuggingFaceH4/ultrafeedback_binarized"
  split: "train_prefs"
  batch_size: 4
  max_length: 1024
  num_workers: 1
  # float for fraction; int for count.
  subset_size: 0.1

optimizer:
  lr: 5.0e-6
  weight_decay: 0.01

trainer:
  sample_during_eval: true
  epochs: 1
  beta: 0.1
  warmup_steps: 100
  max_grad_norm: 1.0
  logging_steps: 10
  save_steps: 500
  eval_steps: 250
  push_to_hub: false
  repo_id: "belati/pythia160m_dpo"
  gradient_accumulation_steps: 1
  max_length: 512
