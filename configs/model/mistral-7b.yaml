# @package model
name: Mistral-7B-Instruct-v0.2
device: "cuda"
attn_implementation: "flash_attention_2" #kernels-community/flash-attn3
name_or_path: "mistralai/Mistral-7B-Instruct-v0.2"
use_bf16: true
